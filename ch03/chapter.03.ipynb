{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 パーセプトロンからニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 ニューラルネットワークの例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークを図で表すと, 図3-1のようになります.  \n",
    "ここで一番左の列を**入力層**, 一番右の列を**出力層**, 中間の列を**中間層**と呼びます.  \n",
    "中間層は**隠れ層**と呼ぶこともあります\n",
    "「隠れ」という言葉は, 隠れ層のニューロンが(入力層や出力層とは違って)人の目には見えない, ということを表しています.  \n",
    "ここでは, 入力層から出力層へ向かって, 順に第0層, 第1層, 第2層と呼ぶことにします.  \n",
    "図3-1では, 第0層が入力層, 第1層が中間層, 第2層が出力層に対応することになります.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-1を見る限り,前章で見たパーセプトロンと同じような形をしています.  \n",
    "実際, ニューロンの\"つながり方\"に関して言えば, 前章で見たパーセプトロンと何ら変わりません.  \n",
    "それでは,ニューラルネットワークではどのように信号を伝達するのでしょうか.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 パーセプトロンの復習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これからニューラルネットワークにおける信号の伝達方法を見ていきますが, それに先立ち,ここではパーセプトロンの復習から始めたいとおもいます.  \n",
    "それでは,初めに,図3-2の構造のネットワークを考えましょう.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-2は$x_1$と$x_2$の2つの入力信号を受け取り, $y$を出力するパーセプトロンです.  \n",
    "図3-2のパーセプトロンを数式で表すと, 次の式(3.1)で表せるのでした.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{y} =\n",
    "        \\begin{cases}\n",
    "            0 \\quad ( b + w_1x_1 + w_2x_2 \\leqq 0 ) \\\\\n",
    "            1 \\quad ( b + w_1x_1 + w_2x_2 > 0 ) \\\\\n",
    "        \\end{cases}\n",
    "    \\tag{3.1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで,$b$は「バイアス」と呼ばれるパラメータで,これはニューロンの発火のしやすさをコントロールします.  \n",
    "一方, $w_1$や$w_2$は各信号の「重み」を表すパラメータで, これらは各信号の重要性をコントロールします.  \n",
    "ところで, 図3-2のネットワークにはバイアス$b$が図示されていません.  \n",
    "もしバイアスを明示するならば, 図3-3のように表すことができます.  \n",
    "図3-3では,重みが$b$で入力が1の信号が追加されています.  \n",
    "このパーセプトロンの動作は, $x_1$と$x_2$と1の3つの諡号がニューロンの入力となり, それら3つの信号にそれぞれの重みが乗算され, 次のニューロンに送信されます.  \n",
    "次のニューロンでは,それらの重み付けされた信号の和が計算され, その和が0を超えたら1を出力し, そうでなければ0を出力します.  \n",
    "ちなみに,バイアスの入力信号は常に1であるため, 図で表す際には, ニューロンを灰色で塗りつぶし, 他のニューロンと差別化することにします.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは,式(3.1)をよりシンプルな形に書き換えたいと思います.  \n",
    "式(3.1)を簡略化するためには, 場合分けの動作---を超えたら1を出力し,そうでなければ0を出力するという動作---をひとつの関数で表します.  \n",
    "ここでは$h(x)$という新しい関数を導入し, 式(3.1)を次の(3.2), (3.3)のように書き換えます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{y} = h(b + w_1x_1 + w_2x_2)\n",
    "    \\tag{3.2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{h(x)} =\n",
    "        \\begin{cases}\n",
    "            0 \\quad ( x \\leqq 0 ) \\\\\n",
    "            1 \\quad ( x > 0 ) \\\\\n",
    "        \\end{cases}\n",
    "    \\tag{3.3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.2)は,入力信号の総和が$h(x)$という関数によって変換され, その変換された値が出力$y$になるということをあらわしています.  \n",
    "そして, 式(3.3)であらわされる$h(x)$関数は入力が0を超えたら1を返し, そうでなければ0を返します.  \n",
    "そのため, 式(3.1)と式(3.2),(3.3)は同じことを行ってます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 活性化関数の登場"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで$h(x)$という関数ですが,このような関数---入力信号の総和を出力信号に変換する関数---は,一般に**活性化関数**(activation function)とよばれます.  \n",
    "「活性化」という名前が意味するように, 活性化関数は入力信号の総和がどのように活性化するか(どのように発火するか)とうことを決定する役割があります.  \n",
    "それではさらに式(3.2)を書き換えていきます.  \n",
    "式(3.2)では,重み付きの入力信号の総和を計算し, そして, その和が活性化関数によって変換される, という2段階の処理を行っています.  \n",
    "そのため, 式(3.2)を丁寧に書くとすれば, 次の2つの式に分けて書くことができます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{a} = b + w_1x_1 + w_2x_2\n",
    "    \\tag{3.4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{y} = h(a)\n",
    "    \\tag{3.5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.4)では,重み付き入力信号とバイアスの総和を計算し, それを$a$とします.  \n",
    "そして,式(3.5)において, $a$が$h()$で変換され, $y$が出力される, という流れになります.  \n",
    "さて, これまでニューロンはひとつの○で図示してきましたが, 式(3.4)と式(3.5)を明示的に示すとすれば, 次の図3-4のように表すことができます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-4で表されるように, これまでのニューロンの○の中に,活性化関数によるプロセスを明示的に図示しています.  \n",
    "つまり, 重み付き信号の和の結果が$a$というノードになり, そして, 活性化関数$h()$によって$y$というノードに変換される,ということがはっきりと示されているわけです.  \n",
    "なを,ここでは,「ニューロン」と「ノード」という用語を同じ意味で用います.  \n",
    "ここで, $a$と$y$の○を「ノード」と呼んでいますが, これは, これまでの「ニューロン」と同じ意味で用いています.  \n",
    "\n",
    "それでは続いて活性化関数について詳しく見ていくことにします.  \n",
    "この活性化関数が, パーセプトロンからニューラルネットワークへ進むための架け橋になります.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.3)で表される活性化関数は, 閾値を境にして出力が切り替わる関数で, それは「ステップ関数」や「階段関数」と呼ばれます.  \n",
    "そのため, 「パーセプトロンでは, 活性化関数にステップ関数を利用している」ということができます.  \n",
    "つまり, 活性化関数の候補としてたくさんある関数の中で, パーセプトロンは「ステップ関数」を採用しているのです.  \n",
    "パーセプトロンは活性化関数にステップ関数を用いているならば, 活性化関数にステップ関数以外の関数を使ったらどうなるのか.  \n",
    "実は活性化関数をステップ関数から別の関数に変更することで, ニューラルネットワークの世界へとすすむことができます.  \n",
    "ニューラルネットワークで利用される活性化関数を紹介する.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 シグモイド関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークでよく用いられるか成果関数のひとつは, 式(3.6)で表される**シグモイド関数**(sigmoid function)です."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{h(x)} = \\frac{ 1 }{ 1 + exp(-x) }\n",
    "    \\tag{3.6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.6)の$exp(-x)$は$e^{-x}$を意味します.  \n",
    "$e$はネイピア数の2.7182…の実数を表します.  \n",
    "式(3.6)で表されるシグモイド関数は一見複雑そうですが、 シグモイド関数も単なる「関数」です --- 関数は,何か入力を与えれば, 何らかの出力が返される変換器です.  \n",
    "たとえば, シグモイド関数に1.0や2.0を入力すると,$h(1.0) = 0.731…$, $h(2.0) = 0.880…$のように, ある値が出力されます.  \n",
    "ニューラルネットワークでは, 活性化関数にシグモイド関数を用いて信号の変換を行い, その変換された信号が次のニューロンに伝えられます.  \n",
    "実は, 前章で見たパーセプトロンとこれから見ていくニューラルネットワークの主な違いは, この活性化関数だけなです.  \n",
    "その他の点 --- ニューロンが多層につながる構造や, 信号の伝達方法 --- は基本的に前章のパーセプトロンと同じです.  \n",
    "それでは, 活性化関数として利用されるシグモイド関数について, ステップ関数と比較しながら詳しくみていくことにしましょう.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 ステップ関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは, Pythonを使ってステップ関数をグラフで表します(関数の形を視覚的に確認することは, 関数を理解する上で重要です.)  \n",
    "ステップ関数は, 式(3.3)で表されるように, 入力が0を超えたら1を出力し, それ以外は0を出力する関数でした.  \n",
    "ステップ関数を単純に実装するとするならば, 次のようになるでしょう.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この実装は単純でわかりやすいのですが, 引数の x は実数(浮動小数点数)しか入力することができません.  \n",
    "つまり, step_function(3.0)といった使い方はできますが, NumPyの配列を引数に取るような使い方---たとえば, step_function(nparray([1.0, 2.0])のような使い方---はできないのです.  \n",
    "ここでは, 今後のことを考え, NumPy配列に対応した実装に修正したいと思います.  \n",
    "そのためには, たとえば, 次のような実装が考えられるでしょう.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の関数の中身はたった2行ですが, NumPyの便利な\"トリック\"を使っているため少しわかりにくいかもしれません.  \n",
    "ここでは, どのようなトリックを使っているのか, 次のPtyhonインタプリタの例を見ながら説明します.  \n",
    "次の例では, x というNumPy配列を用意し, そのNumPy配列に対して不等号による演算を行います.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([-1.0, 1.0, 2.0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x > 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
