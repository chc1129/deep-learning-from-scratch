{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 パーセプトロンからニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 ニューラルネットワークの例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークを図で表すと, 図3-1のようになります.  \n",
    "ここで一番左の列を**入力層**, 一番右の列を**出力層**, 中間の列を**中間層**と呼びます.  \n",
    "中間層は**隠れ層**と呼ぶこともあります\n",
    "「隠れ」という言葉は, 隠れ層のニューロンが(入力層や出力層とは違って)人の目には見えない, ということを表しています.  \n",
    "ここでは, 入力層から出力層へ向かって, 順に第0層, 第1層, 第2層と呼ぶことにします.  \n",
    "図3-1では, 第0層が入力層, 第1層が中間層, 第2層が出力層に対応することになります.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-1を見る限り,前章で見たパーセプトロンと同じような形をしています.  \n",
    "実際, ニューロンの\"つながり方\"に関して言えば, 前章で見たパーセプトロンと何ら変わりません.  \n",
    "それでは,ニューラルネットワークではどのように信号を伝達するのでしょうか.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 パーセプトロンの復習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これからニューラルネットワークにおける信号の伝達方法を見ていきますが, それに先立ち,ここではパーセプトロンの復習から始めたいとおもいます.  \n",
    "それでは,初めに,図3-2の構造のネットワークを考えましょう.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-2は$x_1$と$x_2$の2つの入力信号を受け取り, $y$を出力するパーセプトロンです.  \n",
    "図3-2のパーセプトロンを数式で表すと, 次の式(3.1)で表せるのでした.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{y} =\n",
    "        \\begin{cases}\n",
    "            0 \\quad ( b + w_1x_1 + w_2x_2 \\leqq 0 ) \\\\\n",
    "            1 \\quad ( b + w_1x_1 + w_2x_2 > 0 ) \\\\\n",
    "        \\end{cases}\n",
    "    \\tag{3.1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで,$b$は「バイアス」と呼ばれるパラメータで,これはニューロンの発火のしやすさをコントロールします.  \n",
    "一方, $w_1$や$w_2$は各信号の「重み」を表すパラメータで, これらは各信号の重要性をコントロールします.  \n",
    "ところで, 図3-2のネットワークにはバイアス$b$が図示されていません.  \n",
    "もしバイアスを明示するならば, 図3-3のように表すことができます.  \n",
    "図3-3では,重みが$b$で入力が1の信号が追加されています.  \n",
    "このパーセプトロンの動作は, $x_1$と$x_2$と1の3つの諡号がニューロンの入力となり, それら3つの信号にそれぞれの重みが乗算され, 次のニューロンに送信されます.  \n",
    "次のニューロンでは,それらの重み付けされた信号の和が計算され, その和が0を超えたら1を出力し, そうでなければ0を出力します.  \n",
    "ちなみに,バイアスの入力信号は常に1であるため, 図で表す際には, ニューロンを灰色で塗りつぶし, 他のニューロンと差別化することにします.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは,式(3.1)をよりシンプルな形に書き換えたいと思います.  \n",
    "式(3.1)を簡略化するためには, 場合分けの動作---を超えたら1を出力し,そうでなければ0を出力するという動作---をひとつの関数で表します.  \n",
    "ここでは$h(x)$という新しい関数を導入し, 式(3.1)を次の(3.2), (3.3)のように書き換えます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{y} = h(b + w_1x_1 + w_2x_2)\n",
    "    \\tag{3.2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{h(x)} =\n",
    "        \\begin{cases}\n",
    "            0 \\quad ( x \\leqq 0 ) \\\\\n",
    "            1 \\quad ( x > 0 ) \\\\\n",
    "        \\end{cases}\n",
    "    \\tag{3.3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.2)は,入力信号の総和が$h(x)$という関数によって変換され, その変換された値が出力$y$になるということをあらわしています.  \n",
    "そして, 式(3.3)であらわされる$h(x)$関数は入力が0を超えたら1を返し, そうでなければ0を返します.  \n",
    "そのため, 式(3.1)と式(3.2),(3.3)は同じことを行ってます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 活性化関数の登場"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで$h(x)$という関数ですが,このような関数---入力信号の総和を出力信号に変換する関数---は,一般に**活性化関数**(activation function)とよばれます.  \n",
    "「活性化」という名前が意味するように, 活性化関数は入力信号の総和がどのように活性化するか(どのように発火するか)とうことを決定する役割があります.  \n",
    "それではさらに式(3.2)を書き換えていきます.  \n",
    "式(3.2)では,重み付きの入力信号の総和を計算し, そして, その和が活性化関数によって変換される, という2段階の処理を行っています.  \n",
    "そのため, 式(3.2)を丁寧に書くとすれば, 次の2つの式に分けて書くことができます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{a} = b + w_1x_1 + w_2x_2\n",
    "    \\tag{3.4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{y} = h(a)\n",
    "    \\tag{3.5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.4)では,重み付き入力信号とバイアスの総和を計算し, それを$a$とします.  \n",
    "そして,式(3.5)において, $a$が$h()$で変換され, $y$が出力される, という流れになります.  \n",
    "さて, これまでニューロンはひとつの○で図示してきましたが, 式(3.4)と式(3.5)を明示的に示すとすれば, 次の図3-4のように表すことができます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-4で表されるように, これまでのニューロンの○の中に,活性化関数によるプロセスを明示的に図示しています.  \n",
    "つまり, 重み付き信号の和の結果が$a$というノードになり, そして, 活性化関数$h()$によって$y$というノードに変換される,ということがはっきりと示されているわけです.  \n",
    "なを,ここでは,「ニューロン」と「ノード」という用語を同じ意味で用います.  \n",
    "ここで, $a$と$y$の○を「ノード」と呼んでいますが, これは, これまでの「ニューロン」と同じ意味で用いています.  \n",
    "\n",
    "それでは続いて活性化関数について詳しく見ていくことにします.  \n",
    "この活性化関数が, パーセプトロンからニューラルネットワークへ進むための架け橋になります.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.3)で表される活性化関数は, 閾値を境にして出力が切り替わる関数で, それは「ステップ関数」や「階段関数」と呼ばれます.  \n",
    "そのため, 「パーセプトロンでは, 活性化関数にステップ関数を利用している」ということができます.  \n",
    "つまり, 活性化関数の候補としてたくさんある関数の中で, パーセプトロンは「ステップ関数」を採用しているのです.  \n",
    "パーセプトロンは活性化関数にステップ関数を用いているならば, 活性化関数にステップ関数以外の関数を使ったらどうなるのか.  \n",
    "実は活性化関数をステップ関数から別の関数に変更することで, ニューラルネットワークの世界へとすすむことができます.  \n",
    "ニューラルネットワークで利用される活性化関数を紹介する.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 シグモイド関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークでよく用いられるか成果関数のひとつは, 式(3.6)で表される**シグモイド関数**(sigmoid function)です."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{h(x)} = \\frac{ 1 }{ 1 + exp(-x) }\n",
    "    \\tag{3.6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式(3.6)の$exp(-x)$は$e^{-x}$を意味します.  \n",
    "$e$はネイピア数の2.7182…の実数を表します.  \n",
    "式(3.6)で表されるシグモイド関数は一見複雑そうですが、 シグモイド関数も単なる「関数」です --- 関数は,何か入力を与えれば, 何らかの出力が返される変換器です.  \n",
    "たとえば, シグモイド関数に1.0や2.0を入力すると,$h(1.0) = 0.731…$, $h(2.0) = 0.880…$のように, ある値が出力されます.  \n",
    "ニューラルネットワークでは, 活性化関数にシグモイド関数を用いて信号の変換を行い, その変換された信号が次のニューロンに伝えられます.  \n",
    "実は, 前章で見たパーセプトロンとこれから見ていくニューラルネットワークの主な違いは, この活性化関数だけなです.  \n",
    "その他の点 --- ニューロンが多層につながる構造や, 信号の伝達方法 --- は基本的に前章のパーセプトロンと同じです.  \n",
    "それでは, 活性化関数として利用されるシグモイド関数について, ステップ関数と比較しながら詳しくみていくことにしましょう.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 ステップ関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは, Pythonを使ってステップ関数をグラフで表します(関数の形を視覚的に確認することは, 関数を理解する上で重要です.)  \n",
    "ステップ関数は, 式(3.3)で表されるように, 入力が0を超えたら1を出力し, それ以外は0を出力する関数でした.  \n",
    "ステップ関数を単純に実装するとするならば, 次のようになるでしょう.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この実装は単純でわかりやすいのですが, 引数の x は実数(浮動小数点数)しか入力することができません.  \n",
    "つまり, step_function(3.0)といった使い方はできますが, NumPyの配列を引数に取るような使い方---たとえば, step_function(nparray([1.0, 2.0])のような使い方---はできないのです.  \n",
    "ここでは, 今後のことを考え, NumPy配列に対応した実装に修正したいと思います.  \n",
    "そのためには, たとえば, 次のような実装が考えられるでしょう.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    y = x > 0\n",
    "    return y.astype(np.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の関数の中身はたった2行ですが, NumPyの便利な\"トリック\"を使っているため少しわかりにくいかもしれません.  \n",
    "ここでは, どのようなトリックを使っているのか, 次のPtyhonインタプリタの例を見ながら説明します.  \n",
    "次の例では, x というNumPy配列を用意し, そのNumPy配列に対して不等号による演算を行います.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  2.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([-1.0, 1.0, 2.0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x > 0\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy配列に対して不等号の演算を行うと, 配列の各要素に対して不等号の演算が行われ,ブーリアンの配列が生成されます.  \n",
    "ここでは, x という配列の要素に対し 0より大きい要素はTrureに, 0以下の要素はFalseに変換され, 新しい配列yが生成されます.  \n",
    "さて,先のyという配列はブーリアンの配列でしたが, 私たちの望むステップ関数は, 0か1の「int型」を出力する関数です.  \n",
    "そのため,配列yの要素の方をブーリアンからint型に変換します.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.astype(np.int)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで示したように, NumPy配列の方の返還にはastype()メソッドを用います.  \n",
    "astype()メソッドでは, 引数に希望する型---この例では, np.int---を指定します.  \n",
    "なお, Pythonではブーリアン型からint型に変換すると, Trueが1に, Falseが0に変換されます.  \n",
    "以上が, ステップ関数の実装で使われるNumPyの\"トリック\"の説明でした.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2.3 ステップ関数のグラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "それでは, 上で定義したステップ関数をグラフで表してみましょう.  \n",
    "そのため, ライブラリとしてmatplotlibを使用します.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARc0lEQVR4nO3df4wc513H8c/Hdw6hSpqo8SHAZ8emuFKtpCjoZCLyRwNJkBOCzR8t2ChQIKr/qaFVU5BLUFqlSIhGUIRqKFapWkqpMeFHT8WRKRCEBCTypfkhbNfoZNL64qJc25AipcE3M1/+2L3z6jwzu7Z3d+4Zv19SpJvdx3vfVZ79aO47zzPriBAAIH3rmi4AADAcBDoAtASBDgAtQaADQEsQ6ADQEpNN/eINGzbEli1bmvr1AJCkZ5555hsRMVX2XGOBvmXLFs3NzTX16wEgSba/WvUcLRcAaAkCHQBagkAHgJYg0AGgJQh0AGgJAh0AWoJAB4CWINABoCUIdABoCQIdAFqCQAeAliDQAaAlCHQAaIm+gW77U7Zftv0fFc/b9h/Ynrf9gu0fHn6ZAIB+BjlD/7SknTXP3ytpW/e/fZL+6MrLAgBcqr73Q4+If7G9pWbIbkl/GhEh6SnbN9r+voj4+pBqBBr16mtLeuGl/2m6DLTIm6eu0/ff+N1Df91hfMHFRklne44Xuo9dFOi296lzFq/NmzcP4VcDo/eRvzupx59ZaLoMtMhv/fQteuD2m4f+usMIdJc8FmUDI+KQpEOSNDMzUzoGWGu+/Z0l3XzTG/S77/yhpktBS2y+6Q0jed1hBPqCpE09x9OSzg3hdYE1IS9C1187qZktb2q6FKDWMJYtzkr6he5ql9slvUr/HG2yVIQm1rHCF2tf3zN025+XdKekDbYXJH1I0npJiohPSDoq6T5J85Jek/RLoyoWaEJeFFq/rqyzCKwtg6xy2dvn+ZD0nqFVBKwxS3logkBHAvg7EugjL0KTEwQ61j4CHegjK0KT9NCRAGYp0EeWF5qk5YIEEOhAH3lBDx1pINCBPrIitH6CjwrWPmYp0EeWF5yhIwkEOtBH56IogY61j0AH+mDZIlJBoAN9dDYW8VHB2scsBfrIC5YtIg0EOtBHRssFiSDQgT6ynIuiSAOBDvSRc/tcJIJZCvSRFYXW03JBAgh0oEZRhIoQG4uQBAIdqJEVna++pYeOFBDoQI18OdC5lwsSwCwFaiwVhSTO0JEGAh2okeedM3R66EgBgQ7UyGi5ICHMUqBGRssFCSHQgRoZLRckhEAHaiyvcmFjEVJAoAM1llsubP1HCpilQA02FiElBDpQY7mHTqAjBQQ6UOPCskUCHWsfgQ7UyFeWLfJRwdo30Cy1vdP2advztg+UPL/Z9pO2n7X9gu37hl8qMH5LtFyQkL6BbntC0kFJ90raLmmv7e2rhv2mpCMRcZukPZL+cNiFAk1YXrbIOnSkYJAz9B2S5iPiTEScl3RY0u5VY0LSG7s/3yDp3PBKBJrD1n+kZJBZulHS2Z7jhe5jvT4s6QHbC5KOSvqVsheyvc/2nO25xcXFyygXGK8sZ+s/0jFIoJfN5Fh1vFfSpyNiWtJ9kj5r+6LXjohDETETETNTU1OXXi0wZhktFyRkkEBfkLSp53haF7dUHpR0RJIi4t8lXStpwzAKBJp0Yes/LResfYPM0uOSttneavsadS56zq4a8zVJd0mS7beqE+j0VJC8pXx56z9n6Fj7+gZ6RGSS9ks6JumUOqtZTth+1Pau7rCHJL3b9vOSPi/pFyNidVsGSE7O1n8kZHKQQRFxVJ2Lnb2PPdLz80lJdwy3NKB57BRFSmgMAjUu3MuFjwrWPmYpUCMv6KEjHQQ6UCPjCy6QEAIdqMFX0CElBDpQ48IXXPBRwdrHLAVqrNw+l5YLEkCgAzWWb587YQIdax+BDtTIi9A6S+vooSMBBDpQIyuCW+ciGcxUoEaWF2z7RzIIdKBGVgRLFpEMAh2okRfBrXORDGYqUCMrCs7QkQwCHaiR5UEPHckg0IEaeRFsKkIyCHSgxlIRbPtHMpipQI2cHjoSQqADNeihIyUEOlAjo4eOhBDoQI2MHjoSwkwFarD1Hykh0IEabP1HSgh0oAZb/5ESZipQI8tZtoh0EOhAjc5FUQIdaSDQgRps/UdKCHSgxlJesGwRyRhoptreafu07XnbByrG/Iztk7ZP2P7z4ZYJNCNnlQsSMtlvgO0JSQcl3SNpQdJx27MRcbJnzDZJH5R0R0S8Yvt7RlUwME7sFEVKBjlD3yFpPiLORMR5SYcl7V415t2SDkbEK5IUES8Pt0ygGdzLBSkZJNA3Sjrbc7zQfazXWyS9xfa/2n7K9s6yF7K9z/ac7bnFxcXLqxgYo87GInroSMMgM7Xs9CRWHU9K2ibpTkl7JX3S9o0X/aOIQxExExEzU1NTl1orMHZ5UWg9LRckYpBAX5C0qed4WtK5kjFfiIiliPgvSafVCXggaVnORVGkY5BAPy5pm+2ttq+RtEfS7KoxfyvpxyTJ9gZ1WjBnhlko0AQ2FiElfQM9IjJJ+yUdk3RK0pGIOGH7Udu7usOOSfqm7ZOSnpT0axHxzVEVDYxLZ2MRPXSkoe+yRUmKiKOSjq567JGen0PS+7v/Aa2xVHD7XKSDUw+gQlGEIkQPHckg0IEKWdFZzMXtc5EKZipQISsKSZyhIx0EOlBh+QydHjpSQaADFfKcQEdaCHSgwtJyy4UeOhLBTAUq5LRckBgCHaiQ0XJBYgh0oMLKRVFuzoVEEOhAhXxl2SIfE6SBmQpUWNlYRMsFiSDQgQrLPXQ2FiEVBDpQgR46UkOgAxWWe+iT9NCRCGYqUGGJZYtIDIEOVFjZWMROUSSCmQpUWMq52yLSQqADFdj6j9QQ6EAFVrkgNQQ6UOHCvVz4mCANzFSgAt9YhNQQ6ECFfOU7RQl0pIFAByqw9R+pIdCBChe+U5SPCdLATAUqrGz9p+WCRBDoQAW2/iM1BDpQYfmiKD10pGKgQLe90/Zp2/O2D9SMe4ftsD0zvBKBZqx8wQX3ckEi+s5U2xOSDkq6V9J2SXttby8Zd72kX5X09LCLBJqQcS8XJGaQU48dkuYj4kxEnJd0WNLuknEfkfRRSa8PsT6gMRn3ckFiBgn0jZLO9hwvdB9bYfs2SZsi4ot1L2R7n+0523OLi4uXXCwwTnkRmlhn2QQ60jBIoJfN5lh50l4n6WOSHur3QhFxKCJmImJmampq8CqBBiwVBe0WJGWQQF+QtKnneFrSuZ7j6yXdIumfbb8o6XZJs1wYReryPGi3ICmDBPpxSdtsb7V9jaQ9kmaXn4yIVyNiQ0RsiYgtkp6StCsi5kZSMTAmWUGgIy19Az0iMkn7JR2TdErSkYg4YftR27tGXSDQlKwo+Po5JGVykEERcVTS0VWPPVIx9s4rLwto3vJFUSAVnH4AFbI8tJ5AR0IIdKBCVoQmuDEXEkKgAxU6F0X5iCAdzFagQl4UrHJBUgh0oMJSzkVRpIVAByrkRfDlFkgKgQ5UoIeO1DBbgQpZTg8daSHQgQoZLRckhkAHKnTO0PmIIB3MVqACW/+RGgIdqJAVofW0XJAQAh2okLEOHYkh0IEKWUEPHWlhtgIV2FiE1BDoQAW2/iM1BDpQIecr6JAYAh2o0NlYxEcE6WC2AhUybp+LxBDoQIWcHjoSQ6ADFTobi/iIIB3MVqBCVhScoSMpBDpQIWOVCxJDoAMliiIUIXaKIinMVqDEUlFIEjtFkRQCHSiRFyFJ9NCRFAIdKJF1A50eOlIyUKDb3mn7tO152wdKnn+/7ZO2X7D9j7ZvHn6pwPhkOYGO9PQNdNsTkg5KulfSdkl7bW9fNexZSTMR8TZJj0v66LALBcYp6/bQJ1iHjoQMMlt3SJqPiDMRcV7SYUm7ewdExJMR8Vr38ClJ08MtExiv5R76es7QkZBBAn2jpLM9xwvdx6o8KOmJsids77M9Z3tucXFx8CqBMVtuuXBRFCkZJNDLZnSUDrQfkDQj6bGy5yPiUETMRMTM1NTU4FUCY7ZyUZRli0jI5ABjFiRt6jmelnRu9SDbd0t6WNLbI+L/hlMe0Ix8eR06G4uQkEFm63FJ22xvtX2NpD2SZnsH2L5N0h9L2hURLw+/TGC8lljlggT1DfSIyCTtl3RM0ilJRyLihO1Hbe/qDntM0nWS/tL2c7ZnK14OSAIbi5CiQVouioijko6ueuyRnp/vHnJdQKOWe+jcPhcpYbYCJbK8uw6dM3QkhEAHSrDKBSki0IESF7b+8xFBOpitQImVrf+0XJAQAh0osbL1n5YLEkKgAyWW2PqPBBHoQIm8oIeO9DBbgRIZX0GHBBHoQAm+4AIpItCBEmz9R4oIdKAEW/+RImYrUIJ16EgRgQ6UoIeOFBHoQImVZYu0XJAQZitQYmnlG4s4Q0c6CHSgRM5OUSSIQAdKrNw+l0BHQgh0oERWFJpYZ9kEOtJBoAMlsiJotyA5BDpQIs9D6wl0JIZAB0pwho4UEehAiawoWIOO5DBjgRJ5EaxwQXIIdKDEUk6gIz0EOlAiL0ITfLkFEkOgAyWyIrSer59DYpixQIksL1jlguQQ6EAJli0iRQMFuu2dtk/bnrd9oOT577L9F93nn7a9ZdiFAuOUF8G3FSE5k/0G2J6QdFDSPZIWJB23PRsRJ3uGPSjplYj4Qdt7JP2OpJ8dRcGvL+V6fSkfxUsDK75zPucMHcnpG+iSdkiaj4gzkmT7sKTdknoDfbekD3d/flzSx207ImKItUqSPvNvL+q3n/jKsF8WuMjtP/CmpksALskggb5R0tme4wVJP1I1JiIy269KuknSN3oH2d4naZ8kbd68+bIK/tE3b9CHfmr7Zf1b4FLs2EqgIy2DBHrZ352rz7wHGaOIOCTpkCTNzMxc1tn7rdM36NbpGy7nnwJAqw1y1WdB0qae42lJ56rG2J6UdIOkbw2jQADAYAYJ9OOSttneavsaSXskza4aMyvpXd2f3yHpn0bRPwcAVOvbcun2xPdLOiZpQtKnIuKE7UclzUXErKQ/kfRZ2/PqnJnvGWXRAICLDdJDV0QclXR01WOP9Pz8uqR3Drc0AMClYOcEALQEgQ4ALUGgA0BLEOgA0BIEOgC0BIEOAC1BoANASxDoANASBDoAtASBDgAtQaADQEsQ6ADQEm7qLre2FyV9tZFffmU2aNU3MV0lrsb3zXu+eqT0vm+OiKmyJxoL9FTZnouImabrGLer8X3znq8ebXnftFwAoCUIdABoCQL90h1quoCGXI3vm/d89WjF+6aHDgAtwRk6ALQEgQ4ALUGgXwHbH7Adtjc0Xcuo2X7M9ldsv2D7b2zf2HRNo2R7p+3TtudtH2i6nlGzvcn2k7ZP2T5h+71N1zQutidsP2v7i03XcqUI9Mtke5OkeyR9relaxuRLkm6JiLdJ+k9JH2y4npGxPSHpoKR7JW2XtNf29marGrlM0kMR8VZJt0t6z1Xwnpe9V9KpposYBgL98n1M0q9LuiquKkfE30dE1j18StJ0k/WM2A5J8xFxJiLOSzosaXfDNY1URHw9Ir7c/fl/1Qm4jc1WNXq2pyX9pKRPNl3LMBDol8H2LkkvRcTzTdfSkF+W9ETTRYzQRklne44XdBWE2zLbWyTdJunpZisZi99X58SsaLqQYZhsuoC1yvY/SPrekqcelvQbkn5ivBWNXt17jogvdMc8rM6f558bZ21j5pLHroq/xGxfJ+mvJL0vIr7ddD2jZPt+SS9HxDO272y6nmEg0CtExN1lj9u+VdJWSc/bljqthy/b3hER/z3GEoeu6j0vs/0uSfdLuivavYFhQdKmnuNpSecaqmVsbK9XJ8w/FxF/3XQ9Y3CHpF2275N0raQ32v6ziHig4bouGxuLrpDtFyXNREQqd2q7LLZ3Svo9SW+PiMWm6xkl25PqXPi9S9JLko5L+rmIONFoYSPkztnJZyR9KyLe13Q949Y9Q/9ARNzfdC1Xgh46BvVxSddL+pLt52x/oumCRqV78Xe/pGPqXBw80uYw77pD0s9L+vHu/9/numeuSAhn6ADQEpyhA0BLEOgA0BIEOgC0BIEOAC1BoANASxDoANASBDoAtMT/A8qJLvGmdrBqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = step_function(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1) # y軸の範囲を指定\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-5.0, 5.0, 0.2)は-5.0から5.0までの範囲を0.1刻みでNumpy配列を生成する.  \n",
    "([-0.5, -4.9, …, 4.9]を生成します.)  \n",
    "step_function()はNumPy配列を引数に取り, 配列の各要素に対してステップ関数を実行し, 結果を配列として返します.  \n",
    "この, x, y 配列をプロットすると結果は次の図3-6のようになります.  \n",
    "図3-6で表されるように, ステップ関数は0を境にして,出力が0から1(または, 1から0)へ切り替わります.  \n",
    "図3-6のように階段上に値が切り替わる形からステップ「階段関数」とよばれることもおすすめす.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 シグモイド関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シグモイド関数を実装しましょう.  \n",
    "式(3.6)のシグモイド関数はPythonで次のように書くことができます.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでnp.exp(-x)は数式のexp(-x)に対応します.  \n",
    "この実装は特に難しいことはありませんが, 引数のxにNumPy配列を入力しても, 結果は正しく計算されることに注意しましょう.  \n",
    "実際, この**sigmoid**関数にNumPy配列を入力すると, 次のように正しく計算されます.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894142, 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-1.0, 1.0, 2.0])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シグモイド関数の実装がNumPy配列に対応していることは, NumPyのブロードキャストに秘密があります.  \n",
    "ブロードキャストの機能により, スカラ値とNumPy配列での演算が行われると, スカラ値とNumPy配列の各要素どうしで演算が行われます.  \n",
    "ここでもひとつ具体例を示しましょう."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 4.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([1.0, 2.0, 3.0])\n",
    "1.0 + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 / t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の例えでは, スカラ値(例では, 1.0)とNumPy配列の間で数値演算(+や/など)が行われています.  \n",
    "結果としてスカラ値とNumPy配列の各要素の間で演算が行われ, 演算の結果がNumPy配列として出力されています.  \n",
    "先のシグモイド関数の実装でも, np.exp(-x)はNumPy配列を生成するため, 1 / (1 + np.exp(-x))の結果は, NumPy配列の各要素の間で計算されることになります.  \n",
    "それでは, シグモイド関数をグラフに描画します.  \n",
    "描画のためのコードは, 先のステップ関数のコードとほとんど同じです.  \n",
    "唯一異なる箇所は, yを出力する関数をsigmoid関数に変更する点です.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-5e6a1a26ce7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# y軸の範囲を指定\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1) # y軸の範囲を指定\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 シグモイド関数とステップ関数の比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シグモイド関数とステップ関数を見比べてみましょう.  ステップ関数とシグモイド関数を図3-8に示す.2つの関数のことあっている点はどこでしょうか?  \n",
    "また, どのような点が共通する性質と言えるでしょうか?図3-8を観察して考えてみましょう.  \n",
    "図3-8を見てまず気づく点は「滑らかさ」の違いだと思います.シグモイド関数は滑らかな曲線であり, 入力に対して連続的に出力が変化します.一方ステップ関数は0を境に急に出力を変えています.  \n",
    "このシグモイド関数の滑らかさが, ニューラルネットワークの学習において重要な意味を持ちます.\n",
    "\n",
    "また,ステップ関数が0か1のどちらかの値しか返さないのに対して, シグモイド関数は実数(0.731…や0.880…などを返すという点も異なります.つまり,パーセプトロンではニューロン間を0か1の二値の信号が流れていたのに対して, ニューラルネットワークでは連続的な実数値の信号が流れます.  \n",
    "\n",
    "続いてステップ関数とシグモイド関数の共通する性質についてです.ステップ関数とシグモイド関数では, \"滑らかさ\"という点では異なりますが, 図3-8を大きな視点で見ると同じような形をしていることが分かります.  \n",
    "実際両者とも入力が小さいときに出力は0に近く入力が大きくなるに従い出力が1に近づくという構造をしています. つまり, ステップ関数とシグモイド関数は, 入力信号が重要な情報であれば大きな値を出力し, 入力信号が重要でなければ小さな値を出力するのです. そしてどんなに入力信号の値が小さくても, またどんなに入力信号の値が大きくても, 出力信号の値を0から1の間に押し込めるのも両者の共通点です.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 非線形関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ステップ関数とシグモイド関数の共通点は他にもあります. 重要な共通点は, 両者はともに**非線形関数**であるということです.  \n",
    "シグモイド関数は曲線, ステップ関数は階段のような折れ曲がった直線で表され, ともに非線形な関数に分類されます.  \n",
    "ニューラルネットワークでは, 活性化関数に非線形関数を用いる必要があります.  \n",
    "これは言い換えると, 活性化関数には線形関数を用いてはならない, ということです.  \n",
    "なぜ線形関数を用いてはならないのでしょうか.  \n",
    "それは, 線形関数を用いると, ニューラルネットワークで層を深くすることの意味がなくなってしまうからです.  \n",
    "\n",
    "線形関数の問題点は, どんなに層を深くしても, それと同じことを行う「隠れ層のないネットワーク」が必ず存在する,という事実に起因します. \n",
    "このことを具体的に(やや直観的に)理解するために, 次の簡単な例を考えてみましょう.  \n",
    "ここでは,線形関数である$h(x) = cx$を活性化関数として, $y(x) = h(h(h(x)))$を行う計算を3層のネットワークに対応させて考えることにします.  \n",
    "この計算は, $y(x) = c × c × c$の掛け算を行いますが, 同じことは$y = ax$の1回の掛け算で, つまり, 隠れ層のないネットワークで実現できます.  \n",
    "この例のように、線形関数を用いたっ場合,　多層にすることの利点の移転をi行くことができます  \n",
    "そのため, 層を重ねることの恩恵を得るためには 活性化関数に非線形を使う必要があるのです.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 ReLU関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでに, 活性化関数としてステップ関数とシグモイド関数を紹介しました.  \n",
    "シグモイド関数は, ニューラルネットワークの歴史において, 古くから利用されてきましたが, 最近では**ReLU**(Rectified Linear Unit)という関数が主に用いられます.  \n",
    "ReLUは, 入力が0を超えていれば, その入力をそのまま出力し, 0以下ならば0を出力する関数です.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU関数を数式で表すと, 次の式(3.7)のように書くことができます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{h(x)} =\n",
    "        \\begin{cases}\n",
    "            x \\quad ( x > 0 ) \\\\\n",
    "            0 \\quad ( x \\leqq 0 ) \\\\\n",
    "        \\end{cases}\n",
    "    \\tag{2.2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-55-ee9b6b55fbf0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-55-ee9b6b55fbf0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    グラフや数式の通り, ReLU巻子はとてもシンプルな関数です.\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "グラフや数式の通り, ReLU巻子はとてもシンプルな関数です.  \n",
    "そのため, ReLU関数の実装も簡単で, 次のように書くことができます.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-98c50ba4b0f2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-57-98c50ba4b0f2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ここでは, NumPyのmaximuという関数をつかっています.\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ここでは, NumPyのmaximuという関数をつかっています.  \n",
    "このmaximumは, 入力された値から大きい方の値を選んで出力する関数です.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 多次元配列の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 多次元配列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-58-9733f108fe66>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-58-9733f108fe66>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    多次元配列とは, 簡単に言うと「数字の集合」です.\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "多次元配列とは, 簡単に言うと「数字の集合」です.  \n",
    "数字が1列に並んだものや長方形状に並べたもの, 3次元状に並べたものや(より一般化した)N次元状に並べたものを多次元配列といいます.  \n",
    "それではNumPyを使って, 多次元配列を作成します.  \n",
    "まずは, これまで見てきた1次元の配列からです."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([1, 2, 3, 4])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndim(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで示すように,配列の次元すはnp.ndim()関数で取得できます.  \n",
    "また,配列の形状はインスタンス変数のshapeから取得できます.  \n",
    "上の例では, Aは1次元の配列であり, 4つの要素から構成されていることがわかります.  \n",
    "なお, ここではA.shapeの結果がタプルになっていることに注意しましょう.  \n",
    "これは,1次元配列の場合であっても, 多次元配列の場合と同じ統一された結果を返すからです.  \n",
    "たとえば, 2次元配列のときは(4, 3), 3次元配列のときは(4, 3, 2)といったタプルが返されるため, 次元数が1の1次元配列のときも同様にタプルとして結果がかえされます.  \n",
    "それでは, 続いて2次元の配列を作成します.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[1,2], [3,4], [5,6]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndim(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは「3 × 2 の配列」であるBを作成しています. \n",
    "3 × 2 の配列とは, 最初の次元に2つの要素があり, 次の次元に2つの要素があるという意味です.  \n",
    "なお, 最初の次元には0番目の次元, 次の次元は1番目の次元に対応します(Pythonのインデックスは0から始まります).  \n",
    "また, 2次元配列は**行列**(matrix)と呼びます.  \n",
    "図3-10 に示すように, 配列の横方向の並びを**行**(row), 縦方向の並びを**列**(column)と呼びます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 行列の積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて, 行列(2次元配列)の積について説明します.  \n",
    "行列の積は, たとえば, 2 × 2の行列の場合, 図3-11のように計sなします(次の手順で計算することが定義されています).  \n",
    "この例で示すように, 行列の積は, 左側の行列の行(横方向)と右側の行列の列(縦方向)の間の要素ごとの積とその和によって計算が行われます.  \n",
    "そして, その計算の結果は新しい多次元配列の要素として格納されます.  \n",
    "たとえば, **A**の1行目と**B**の1列目の結果は1行目1列目の要素, **A**の2行目と**B**の1列目は2行1列目の要素といったようになります.  \n",
    "なお, 本書では数式の表記において, 行列は太字で表すことにします.  \n",
    "たとえば, 行列は**A**のように表記し, 要素がひとつのスカラ値(たとえば, a, b)とは区別します.  \n",
    "さて, この計算をPythonで実装すると次のようになります.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2], [3,4]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[5,6], [7,8]])\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで, AとBは2 × 2の行列であり, **A**と**B**の行列の積をNumPyの関数**np.dot()**\\で計算します(この**dot**は「ドット積」を意味します).  \n",
    "**np.dot**(ドット積)は, 1次元配列の場合はベクトルを, 2次元配列では行列の積を計算します.  \n",
    "ここで注意が必要なのは, **np.dot(A, B)**\\と**np.dot(B, A)**\\は異なる値になりえるということです.  \n",
    "通常の演算(+や\\*など)と違って, 行列の積では, 被演算子(A, B)の順番が異なると, 結果も異なります.  \n",
    "さて, ここでは2 × 2の形状の行列について, その積を求める例を示しましたが, 別の形状の行列の積についても, 同様に計算することができます.  \n",
    "たとえば, 2 × 3の行列と3 × 2の行列の積をPythonで実装すると次のようになります.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[1,2], [3,4], [5,6]])\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 28],\n",
       "       [49, 64]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 × 3の行列**A**と3 × 2の行列**B**の積は上のように実装できます.  \n",
    "ここで注意するべき点は「行列の形状(**shape**)」についてです.  \n",
    "具体的に言うと, 行列**A**の1次元目の要素数(列数)と行列**B**の0次元目の要素数(行数)を同じ値にする必要があります.  \n",
    "実際に, 上の例では, 行列**A**は2 × 3, 行数**B**は3 × 2であり, 行列**A**の1次元目の要素数(3)と行列**B**の0次元目の要素数(3)は同じ値です.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.array([[1,2], [3,4]])\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-bb5afb89b162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(A, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このエラーが述べていることは, 行列**A**の1次元目の行列**C**の0次元目の次元の要素数が一致していない, ということです(次元のインデックスは0番目から始まります).  \n",
    "つまり, 多次元配列の積では, 2つの行列で対応する次元の要素数を一致させる必要があるということです.  \n",
    "これは大切な点なので,図3-12でもう一度確認しましょう."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-12には, 3 × 2の行列**A**と2 × 4の行列**B**の積によって, 3 × 4の行列**C**が生成される例が示されています.  \n",
    "この図が示すように, 行列**A**と**B**の対応する次元の要素数は一致させる必要があります.  \n",
    "そして,計算結果である行列**C**は, 行列**A**の行数と行列**B**の列数から構成されます---これも重要な点です.  \n",
    "なお, **A**が2次元の行列で, **B**が1次元の配列の場合でも, 次の図3-13で示すように, 「対応する次元の要素数を一致させる」という同じ原則が成り立ちます.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-13の例をPythonで実装すると次のようになります."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2], [3, 4], [5, 6]])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([7, 8])\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 53, 83])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 ニューラルネットワークの行列の積"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは, NumPy行列を使ってニューラルネットワークの実装を行いましょう.  \n",
    "ここでは図3-14の簡単なニューラルネットワークを対象とします.  \n",
    "このニューラルネットワークは, バイアスと活性化関数は省略し, 重みだけがあるものとします.  \n",
    "実装に関しては, **X**, **W**, **Y**の形状に注意しましょう.  \n",
    "特に, **X**と**W**の対応する次元の要素数が一致していることが重要な点です.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([1, 2])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 5]\n",
      " [2 4 6]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1, 3, 5], [2, 4, 6]])\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 11 17]\n"
     ]
    }
   ],
   "source": [
    "Y = np.dot(X, W)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで示したように,np.dot(多次元のドット積)を使えば, Yの結果を一度に計算することができます.  \n",
    "これが意味することは、 もしYの要素数が100や1000であったとしても, 一度の演算で計算するできるということです\n",
    "もし, np.dotを使わなければ, Yの要素をひとつずつ取り出して計算しなければならない(または, for文お使って計算をしなければならない)ので, とても面倒です. \n",
    "そのため, 行列の積によって一度で計算ができるというテクニックは, 実装上とても重要であると言えます.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 3層ニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは, \"実践的\"なニューラルネットワークの実装を行いましょう.  \n",
    "ここでは, 図3-15に示す3層ニューラルネットワークを対象として, その入力から出力への処理(フォワード方向への処理)を実装します.  \n",
    "実装に関しては, 前節で説明したNumPyの多次元配列を使います.  \n",
    "NumPy配列を上手く使う事で, ほんの少しのコードでニューラルネットワークのフォワード処理を完成させることができます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 記号の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではニューラルネットワークで行う処理を説明するにあたって, w12(1)やa1(1)などの記号を導入します. \n",
    "やや込み入った印象を受けるかもしれませんが, これらの記号は本節だけで使用するものなので, 軽く読み飛ばしてもらっても問題ありません.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは,初めに記号の定義から始めます. \n",
    "次の図3-16を見てください. \n",
    "図3-16は入力層のx2のニューロンから, 次層のニューロンa1(1)への重みだけをピックアップして図示しています.\n",
    "図3-16に示すとおり, 重みや隠れ層のニューロンの右上には「(1)」とあります. \n",
    "これは,第1層の重み,第1層のニューロン,ということを意味する番号です. \n",
    "また, 重みの右下には2つ数字が並びますが, これは, 次層のニューロンと前層のニューロンのインデックス番号から構成されます.  \n",
    "例えば, w12(1)は前層の2番目のニューロン(x2)から次層の1番目のニューロン(a1(1))への重みであることを意味します.  \n",
    "重み右下のインデックス番号は「次層の番号, 前層の番号」の順に並ぶことにします.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 各層における信号伝達の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは, 入力層から「第1層目の1番目のニューロン」への信号の伝達を見ていきます. \n",
    "図3-17のようになります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-17に示す通り, バイアスのためのニューロンである①が追加されています.\n",
    "ここでは, バイアスの右下のインデックスがひとつしかないことに注意しましょう.\n",
    "これは前層のバイアスニューロン(①ニューロン)がひとつだけしか存在しないためです. \n",
    "それではこれまでの確認も含めて, a1(1)を数式で表しましょう. \n",
    "a1(1)は重み付き信号とバイアスの和で次のように計算されます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{a_1^{(1)}} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)}\n",
    "    \\tag{3.8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また, 行列の積を用いると, 第1層目の「重み付き和」は次の式でまとめて表すことができます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{A^{(1)}} = XW^{(1)} + B^{(1)}\n",
    "    \\tag{3.9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただし,\n",
    "$ A^{(1)} $ , $ X $ , $ B^{(1)} $ , $ W^{(1)} $\n",
    "は下記の通りです.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{A^{(1)}} = \\Biggl( a_1^{(1)} a_2^{(1)} a_3^{(1)} \\Biggr)\n",
    "    \\\n",
    "$$\n",
    ", \n",
    "$$\n",
    "    \\mathrm{ X } = \\Biggl( x_1 x_2 \\Biggr)\n",
    "    \\\n",
    "$$\n",
    ",\n",
    "$$\n",
    "    \\mathrm{B^{(1)}} = \\Biggl( b_1^{(1)} b_2^{(1)} b_3^{(1)} \\Biggr)\n",
    "    \\\n",
    "$$\n",
    ", \n",
    "$$\n",
    "    \\mathrm{W^{(1)}} = \\begin{pmatrix}\n",
    "        w_{11}^{(1)} & w_{21}^{(1)} &  w_{31}^{(1)} \\\\\n",
    "        w_{12}^{(1)} & w_{22}^{(1)} &  w_{32}^{(1)}\n",
    "    \\end{pmatrix}\n",
    "    \\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは, NumPyの多次元配列を使って, 数式(3.9)を実装しましょう(ここでは, 入力信号, 重み, バイアス,は適当な値に設定しています). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(W1.shape) # (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) # (2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(B1.shape) # (3, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A1 = np.dot(X, W1) + B1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この計算は前節で行った計算と同じです. \n",
    "Wは2 × 3の配列, Xは要素数が2の1次元配列です.  \n",
    "ここでもやはり, W1とXの対応する次元の要素数が一致しています.  \n",
    "続いて,第1層目の活性化関数によるプロセスを見ていきます.  \n",
    "この活性化関数によるプロセスを図で表すと, 次の図3-18のようになります.  \n",
    "図3-18に示す通り, 隠れ層での重み付き和(重み付き信号とバイアスの総和)を$a$で表し, 活性化関数で変換された信号を$z$で表すことにします. \n",
    "また, 図では活性化関数を$h()$で表し, ここではシグモイド関数を使うことにします. \n",
    "これをPythonで実装すると, 次のようになります."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z1 = sigmoid(A1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.dot(X, W1) + B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = sigmoid(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7 1.1]\n"
     ]
    }
   ],
   "source": [
    "print(A1) # [0.3 0.7 1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "print(Z1) # [0.57444252 0.66818777 0.75026011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "このsigmoid()関数は, 前に定義した関数です. \n",
    "この関数は, NumPy配列を受け取り, 同じ要素数からなるNumPyヒア列を返します."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "それでは続いて, 第1層から第2層目までの実装を行います. (図3-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.array([[1.0, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(Z1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(B2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A2 = np.dot(Z1, W2) + B2**  \n",
    "**Z2 = sigmoid(A2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この実装は, 第1層の出力(**Z1**)が第2層への入力になっている点を除けば, 先程の実装とまったく同じです.\n",
    "NumPy配列を使う事で, 層から層への信号の伝達が簡単に書けることがわかります. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に, 第2層目から出力層への信号の伝達です(図3-20).\n",
    "出力層の実装も, これまでの実装とほとんど同じです. \n",
    "ただし最後の活性化関数だけあ, これまでの隠れ層とは異なります. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = np.dot(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_function(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = np.dot(Z2, W3) + B3\n",
    "Y = identity_function(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは, identity_function()という関数を定義して, この関数--これを「恒等kな数」と言います--を出力層の活性化関数として利用します. \n",
    "恒等関数は, 入力をそのまま出力する関数です. \n",
    "そのため, この例では, わざわざidentity_function()を定義する必要はないのですが, これまでの流れと統一するため, このような実装にしています. \n",
    "なお, 図3-20の表記では, 出力層の活性化関数は$\\sigma()$で表し, 隠れ層の活性化関数$h()$とは異なることを示しています\n",
    "($\\sigma$は「シグマ」と言います). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 実装のまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで3層ニューラルネットワークの説明は終わりです. \n",
    "それでは, ここまで行ってきた実装をまとめて書いてみることにします.  \n",
    "なお, ここでは, ニューラルネットワークの実装の慣例といて, 重みだけを**W1**といったように大文字で表記し, それ以外(バイアスや中間結果など)は小文字で表記します.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(newtwork, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは, init_network(), forward()という関数を定義しています \n",
    "init_network()関数で, 重みとバイアスの初期化を行い, それらをディクショナリ型の変数networkに格納します. \n",
    "このディクショナリ型の変数networkには, それぞれの層で必要なパラメータ--重みとバイアス--が格納されています. \n",
    "そして, forward()関数では, 入力信号が出力へと変換されるプロセスがまとめて実装されています. \n",
    "なお, ここでforwardという単語が出てきましたが, これは入力から出力方向への伝達処理を表しています. \n",
    "後ほど, ニューラルネットワーク学習を行う際に, バックワード(backword)方向--出力から入力方向--の処理について見ていく予定です. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで, ニューラルネットワークのフォワード方向の実装は終わりです. \n",
    "NumPyの多次元配列をうまく使うことで, ニューラルネットワークの実装を効率的に行うことができました. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 出力層の設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは, 分類問題と回帰問題の両方に用いることができます. \n",
    "ただし, 分類問題と回帰問題のどちらに用いるかで, 出力層の活性化関数を変更する必要があります. \n",
    "一般的に, 回帰問題で恒等関数を, 分類問題でソフトマックス関数を使用します. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 恒等関数とソフトマックス関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恒等関数は, 入力をそのまま出力します \n",
    "入ってきたものに対して何も手を加えずに出力する関数--それが恒等関数です. \n",
    "そのため, 出力層で恒等関数をもちるときは, 入力信号をそのまま出力するだけになります. \n",
    "なお, 恒等関数によるプロセスをこれまで見てきたニューラルネットワークの図で表すとすれば, 図3-21のように書くことができます. \n",
    "恒等関数によって変換されるプロセスは, これまでの隠れ層での活性化関数と同じで, 1本の矢印で描画します. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一方, 分類問題で使われるソフトマックス関数は, 次の式で表されます."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathrm{y_k} = \\frac{\\exp(a_k)}{\\sum_{i=1}^n\\exp(a_i)}\n",
    "    \\tag{3.10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$exp(x)$は, $e^x$を表す指数関数です. ($e$は2.718…のネイピア数)\n",
    "ここでは出力層が全部で$n$個あるとして, $k$番目の出力$y_k$を求める計算式をあらわしています. \n",
    "式(3.10)に示すように, ソフトマックス関数の分子は入力信号$a_k$の指数関数, 分母はすべての入力信号の指数関数の和から構成されます. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお, ソフトマックス関数を図で表すと, 次の図3-22のようになります. \n",
    "図に示すように, ソフトマックスの出力は, すべての入力信号から矢印による結びつきがあります.  \n",
    "式(3.10)の分母から分かるように, 出力の各ニューロンが, すべての入力信号から影響を受けることになるからです.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは, ソフトマックス巻子を実装しましょう. \n",
    "ここではPythonインタプリタを使って, ひとつずつ結果を確認しながらすすみたいと思います. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.34985881 18.17414537 54.59815003]\n"
     ]
    }
   ],
   "source": [
    "exp_a = np.exp(a)\n",
    "print(exp_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.1221542101633\n"
     ]
    }
   ],
   "source": [
    "sum_exp_a = np.sum(exp_a)\n",
    "print(sum_exp_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "y = exp_a / sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "この実装は, 式(3.10)のソフトマックス関数をそのままPythonで表現したものです. \n",
    "そのため, 特に解説はひつようないでしょう. \n",
    "ここでは, 後ほどソフトマックス関数を使う事を考えてPythonの関数として次のように定義することにします. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
